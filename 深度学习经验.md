# 深度学习通用经验
一些建议可能并不适用，甚至可能对于特定的任务来说是不好的建议，所以请谨慎使用！
[TOC]
## 使用ADAM优化器
使用 ADAM 优化器。它确实很有效，相对于较传统的优化器（如原版梯度下降），我们更喜欢使用 ADAM。在 TensorFlow 环境下使用 ADAM 时，请注意：如果你想要保存和恢复模型权重，请记住在设置完 AdamOptimizer 后设置 Saver，这是因为 ADAM 也有需要恢复的状态（即对应于每个权重的学习率）。
## 使用ReLu激活函数
ReLU 是最好的非线性（激活函数），这就好比 Sublime 是最好的文本编辑器。但说实话，ReLU 确实是运行速度最快、最简便的，而且令人惊讶的是，它们在工作时梯度并不会逐渐减小（从而能够防止梯度消失）。尽管 sigmoid 是一个常用激活函数，但是它在 DNN 中传播梯度的效果并不太好。
## 不要在输出层使用激活函数
这应该是显而易见的，但是如果你通过一个共用的函数构建每一层，那这可能是一个很容易犯的错误：请确保在输出层不要使用激活函数。
## 记得偏置项
为每一层添加一个偏置项。这是机器学习的入门知识：本质上，偏置项将一个平面转换到最佳拟合位置。在 y=mx+b 式中，b 是偏置项，使直线能够向上或向下移动到最佳的拟合位置。
## 使用He初始化
使用方差缩放初始化。在 TensorFlow 中，该方法写作 **tf.contrib.layers.variance\_scaling_initializer()**。根据我们的实验，这种初始化方法比常规高斯分布初始化、截断高斯分布初始化及 Xavier 初始化的泛化/缩放性能更好。粗略地说，方差缩放初始化根据每一层输入或输出的数量（在 TensorFlow 中默认为输入的数量）来调整初始随机权重的方差，从而帮助信号在不需要其他技巧（如梯度裁剪或批归一化）的情况下在网络中更深入地传播。Xavier 和方差缩放初始化类似，只不过 Xavier 中每一层的方差几乎是相同的；但是如果网络的各层之间规模差别很大（常见于卷积神经网络），则这些网络可能并不能很好地处理每一层中相同的方差。
## 白化
白化（归一化）输入数据。在训练中，令样本点的值减去数据集的均值，然后除以它的标准差。当网络的权重在各个方向上延伸和扩展的程度越小，你的网络就能更快、更容易地学习。保持数据输入以均值为中心且方差不变有助于实现这一点。你还必须对每个测试输入也执行相同的归一化过程，所以请确保你的训练集与真实数据类似。
## 以合理地保留动态范围的方式对输入数据进行缩放
这个步骤和归一化有关，但是应该在归一化操作之前进行。例如，在真实世界中范围为 [0, 140000000] 的数据 x 通常可以用「tanh(x)」或「tanh(x/C)」来进行操作，其中 C 是某个常数，它可以对曲线进行拉伸，从而在 tanh 函数的动态倾斜（斜率较大）部分对更大输入范围内的数据进行拟合。尤其是在输入数据在函数的一端或者两端都不受限的时候，神经网络将在数据处于 (0,1) 时学习效果更好。
## 在ADAM中不使用学习率衰减
一般不要使用学习率衰减。在随机梯度下降（SGD）中，降低学习率是很常见的，但是 ADAM 天然地就考虑到了这个问题。如果你真的希望达到模型性能的极致，请在训练结束前的一小段时间内降低学习率；你可能会看到一个突然出现的很小的误差下降，之后它会再次趋于平缓。
## 控制滤波器的数目
如果你的卷积层有 64 或 128 个滤波器，这就已经足够了。特别是对于深度网络来说，比如 128 个滤波器就已经很多了。如果你已经拥有了大量的滤波器，那么再添加更多的滤波器可能并不会提升性能。
## 池化
池化是为了变换不变性（transform invariance）。池化本质上是让网络学习到图像「某个部分」的「一般概念」。例如，最大池化能够帮助卷积网络对图像中特征的平移、旋转和缩放具备一定的鲁棒性。
## 神经网络的调试
如果网络学习效果很差（指网络在训练中的损失/准确率不收敛，或者你得不到想要的结果），你可以试试下面的这些秘诀：
### 过拟合
如果你的网络学习效果不佳，你首先应该做的就是去过拟合一个训练数据点。准确率基本上应该达到 100% 或 99.99%，或者说误差接近 0。如果你的神经网络不能对一个数据点达到过拟合，那么模型架构就可能存在很严重的问题，但这种问题可能是十分细微的。如果你可以过拟合一个数据点，但是在更大的集合上训练时仍然不能收敛，请尝试下面的几条建议。
### 降低学习率
你的网络会学习地更慢，但是它可能会找到一个之前使用较大的步长时没找到的最小值。（直观地说，你可以想象一下你正在走过路边的沟渠，此时你想要走进沟的最深处，在那里模型的误差是最小的。）

### 提高学习率
这将加快训练速度，有助于加强反馈回路（feedback loop）。这意味着你很快就能大概知道你的网络是否有效。尽管这样一来网络应该能更快地收敛，但是训练结果可能不会太好，而且这种「收敛」状态可能实际上是反复震荡的。（使用 ADAM 优化器时，我们认为在许多实验场景下，~0.001 是比较好的学习率。）

### 减小（小）批量处理的规模
将批处理大小减小到 1 可以向你提供与权重更新相关的更细粒度的反馈，你应该将该过程在 TensorBoard（或者其他的调试/可视化工具）中展示出来。

### 删掉批归一化层
在将批处理大小减小为 1 时，这样做会暴露是否有梯度消失和梯度爆炸等问题。我们曾经遇到过一个好几个星期都没有收敛的网络，当我们删除了批归一化层（BN 层）之后，我们才意识到第二次迭代的输出都是 NaN。在这里使用批量归一化层，相当于在需要止血带的伤口上贴上了创可贴。批归一化有它能够发挥效果的地方，但前提是你确定自己的网络没有 bug。

### 加大（小）批量处理的规模
使用一个更大的批处理规模——还觉得不够的话，如果可以，你不妨使用整个训练集——能减小梯度更新的方差，使每次迭代变得更加准确。换句话说，权重更新能够朝着正确的方向发展。但是！它的有效性存在上限，而且还有一些物理内存的限制。我们发现，这条建议通常不如前两个建议（将批处理规模减小到 1、删除批归一化层）有用。

### 检查你矩阵的重构「reshape」
大幅度的矩阵重构（比如改变图像的 X、Y 维度）会破坏空间局部性，使网络更不容易学习，因为这时网络也必须学习重构。（自然特征变得支离破碎。事实上自然特征呈现出空间局部性也是卷积神经网络能够如此有效的原因！）使用多个图像/通道进行重构时要特别小心；可以使用 numpy.stack() 进行适当的对齐操作。

### 仔细检查你的损失函数
如果我们使用的是一个复杂的函数，可以试着把它简化为 L1 或 L2 这样的形式。我们发现 L1 对异常值不那么敏感，当我们遇到带有噪声的批或训练点时，可以进行稍小幅度的调整。
