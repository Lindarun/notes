# 机器学习
[TOC]
## 根据training cost来确定学习速率
这一切似乎都很自然。然而，使用训练误差来选择η似乎与我在本节前面所说的相矛盾，即我们通过使用我们保留的验证数据评估性能来选择超参数。实际上，我们将使用验证精度(验证集误差)来选择正则化超参数，小批量大小和网络参数，例如层数和隐藏神经元等。为什么学习率会有所不同？坦率地说，这种选择是我个人的审美偏好，也许有点特殊。原因是其他超参数旨在提高测试集的最终分类精度，因此在验证准确性的基础上选择它们是有意义的。但是，学习率只是偶然意味着影响最终的分类准确性。它的主要目的是控制梯度下降中的步长，监测训练成本是检测步长是否过大的最佳方法。话虽如此，这是个人的审美偏好。在学习的早期阶段，如果验证准确度提高，训练误差通常会降低，因此在实践中，您使用哪个标准不太可能产生太大差异。
## Early Stopping
所谓early stopping，即在每一个epoch结束时（一个epoch即对所有训练数据的一轮遍历）计算 validation data的accuracy，当accuracy不再提高时，就停止训练。这是很自然的做法，因为accuracy不再提高了，训练下去也没用。另外，这样做还能防止overfitting。

那么，怎么样才算是validation accuracy不再提高呢？并不是说validation accuracy一降下来，它就是“不再提高”，因为可能经过这个epoch后，accuracy降低了，但是随后的epoch又让accuracy升上去了，所以不能根据一两次的连续降低就判断“不再提高”。正确的做法是，在训练的过程中，记录最佳的validation accuracy，当连续10次epoch（或者更多次）没达到最佳accuracy时，你可以认为“不再提高”，此时使用early stopping。这个策略就叫“ no-improvement-in-n”。
## 学习率退火
在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式：
- **随步数衰减**：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。
- **指数衰减**。数学公式是

```math
\alpha=\alpha_0e^{-kt}
```

> 其中\alpha_0,k是超参数，t是迭代次数（也可以使用周期作为单位）

- **1/t衰减**.数学公式是
```math
\alpha=\alpha_0/(1+kt)
```
> 其中\alpha_0,k是超参数，t是迭代次数。

在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比k更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。
> Adam不推荐使用学习率退火

## 超参数调优
我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：
- 初始学习率
- 学习率衰减方式
- 正则化强度（L2惩罚，随机失活强度）

但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及其时间表的设置等。在本节中将介绍一些额外的调参要点和技巧：

**实现。** 更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要,为这会影响你设计代码的思路。   
一个具体的设计是用**仆程序**持续地随机设置参数然后进行最优化。在训练过程中，**仆程序会对每个周期后验证集的准确率进行监控**，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个**主程序**，**它可以启动或者结束计算集群中的仆程序**，有时候也可能根据条件查看仆程序写下的记录点，输出它们的训练统计数据等。

**比起交叉验证最好使用一个验证集**。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。

**超参数范围**。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：
```
learning_rate = 10 ** uniform(-6, 1)
```
也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.01，那么对于学习进程会有很大影响。然而当学习率是10的时候，影响就微乎其微了。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：dropout=uniform(0,1)）。

**随机搜索优于网格搜索**。
![image](https://pic1.zhimg.com/80/d25cf561835c7b96ae6d1c91868bcbff_hd.jpg)
通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。

**对于边界上的最优值要小心**。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使
```
用learning_rate = 10 ** uniform(-6,1)
```
来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。

**从粗到细地分阶段搜索**。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。

**贝叶斯超参数最优化是一整个研究领域**，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： Spearmint, SMAC, 和Hyperopt。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。这里有更详细的讨论。

## 总结
训练一个神经网络需要：
- 利用小批量数据对实现进行梯度检查，还要注意各种错误。

- 进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。

- 在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。

- 推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。

- 随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。

- 使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。

- 进行模型集成来获得额外的性能提高。

## VC维
### 增长函数
增长函数表示假设空间H对m个示例所能赋予标记的**最大可能结果数**。
> 比如说现在数据集有两个数据点，考虑一种二分类的情况，可以将其分类成A或者B，则可能的值有：AA、AB、BA和BB，所以这里增长函数的值为4。

增长函数值越大则假设空间H的表示能力越强，复杂度也越高，学习任务的适应能力越强。不过尽管H中可以有无穷多的假设h，但是增长函数却不是无穷大的：对于m个示例的数据集，最多只能有2^m 个标记结果，而且很多情况下也达不到2^m的情况。
### dichotomy对分
对于二分类问题来说，H中的假设对D中m个示例赋予标记的每种可能结果称为对D的一种**对分**（dichotomy）。对分也是增长函数的一种上限。
### shatter打散
对于一个给定集合S={x1, ... ,xd}，如果一个假设类H能够实现集合S中所有元素的任意一种标记方式，则称H能够打散S。   
当假设空间H作用于N个input的样本集时，产生的dichotomies数量等于这N个点总的组合数2^N是，就称：这N个inputs被H给shatter掉了。
### VC维
如果我们将假设集合的数量|H|比作假设集合的自由度，那么VC维就是假设集合在做二元分类的有效的自由度，即**这个假设空间能够产生多少Dichotomies的能力**（VC维说的是，到什么时候，假设集合还能shatter，还能产生最多的Dichotomies）。

VC维被认为是数学和计算机科学中非常重要的定量化概念，它可用来刻画分类系统的性能。在机器学习里我们常常看到这样的说法：一般而言, VC维越大, 学习能力就越强,学习也越复杂；可以通过VC维计算学习风险的上界。
![image](http://static.oschina.net/uploads/img/201506/27150629_q2Ej.png)
VC维越大，Ein（随机抽样得到的样本错误率）越小，Ω(N,Η,δ)（模型复杂度）越大

VC维越小，Ω(N,Η,δ)越小，Ein越大

这里一个很直接的信息就是模型复杂度随着VC维的变大不断变大，呈正相关趋势。

因为VC维变大时，数据中可以shatter的点就变多了，那么假设空间的容量就变大了，如果是一个合理的学习算法的话，就有机会找到一个更好的假设，使得Ein更小。

而Eout呢？在一开始的时候，Eout随着Ein的下降也下降，但是到某个点，因为我们要付出的代价变大了，Eout开始上升，所以最好的Eout是在中间的某个位置。

这里得到一个重要的结论**，VC维越大或者假设空间能力越强大，虽然可以使得训练误差很小，但不见得是最好的选择，因为要为模型复杂度付出代价**。

现在，我们用VC维这个工具来描述。
- 如果VC维很小，那么发生预测偏差很大的坏事情的可能性也就很小，那这有利于Ein(g)接近Eout(g)；但是，这是我们的假设空间（H）的**表达能力受到了限制**，这样Ein(g)可能就没有办法做到很小。
- 如果VC维很大，那么假设空间的表达能力很强，我们很有可能选到一个Ein(g)很小的假设，但**是Ein(g)和Eout(g)之差很大的坏事情 发生的情况发生的可能性就变得很大**，这样Ein(g)和Eout(g)根本不接近，我们就无法确定选择的假设在测试数据的时候表现的很好。


对一个指示函数集，如果存在h个样本能够被函数集中的函数按所有可能的2^h种形式分开，则称函数集能够把h个样本打散，函数集的VC维就是它能打散的最大样本数目h，若对任意数目的样本都有函数能将它们打散.则函数集的VC维是无穷大。有界实函数的VC维可以通过用一定的阈值将它转化成指示函数来定义。VC维反映了函数集的学习能力，VC维越大则学习机器越复杂，所以VC维又是学习机器复杂程度的一种衡量。**这时，VC维变成了我们一个重要的选择，我们可以用VC维提供的信息来选择更好的模型和假设空间。**

> 我们可以根据VC Bound公式，设发生坏事情的概率是δ，将其恒等变换可以得到训练误差和测试误差的差别ε。所以反过来讲，好事情（训练误差和测试误差的差别小于ε）发生时，Eout(g)被限制在一个范围内。这里根号内的式子定义为Ω(N,Η,δ)，称作模型复杂度,这个参数描述的意义是，我们的假设空间H有多么的强，使得我们的算法在泛化能力上需要付出多少代价。通俗的来讲，假设空间的容量越大，VC维越大，那么模型就越难学习。
![image](http://static.oschina.net/uploads/space/2015/0627/144802_fl1g_2243330.jpg)

下面举个例子：   
对于三个样本点的情况，下面的S1所有的标记方式是可以使用线性分类器进行分类的，因此其VC维至少为3
![image](http://static.oschina.net/uploads/space/2015/0627/141335_4wrq_2243330.jpg)
虽然存在下面这种情况的S2，其中一种标记方式无法用线性分类器分类
![image](http://static.oschina.net/uploads/space/2015/0627/141453_ClaB_2243330.jpg)

但这种情况并不影响，这是因为，上一种的S1中，我们的H={二维线性分类器}可以实现其所有可能标签情况的分类，这和S2不能用H分散无关。（所以说只要有一种情况能用H分散即可）   
而对于4个样本点的情况，我们的H不能实现其所有可能标签情况的分类（这是经过证明的，过程不详）如下图中某个S和其中一种标签分配情况：
![image](http://static.oschina.net/uploads/space/2015/0627/141612_uM4y_2243330.jpg)

可见，H={二维线性分类器}的VC维是3。另外N维实数空间中线性分类器和线性实函数的VC维是n+1。对一些特殊的函数我们也明确知道其VC维，但并不是所有的。对于任意函数目前还没有很好的指导性方法来计算其VC维。

如果某函数的VC维无穷大，也就意味着，任意多个点无论怎样标注都能将其打散。例如sin(ax)。它可以将任意多样本的任意标注情况精确分开，即在训练集上达到100%的分类正确率。