[TOC]
# TensorFlow
## 数据读取
### Feeding
当TensorFlow运行每步计算的时候，从Python获取数据。在Graph的设计阶段，用placeholder占住Graph的位置，完成Graph的表达；当Graph传给Session后，在运算时再把需要的数据从Python传过来。

### Preloaded data
数据直接预加载到TensorFlow的Graph中，再把Graph传入Session运行。只适用于小数据。

### Reading from file
在Graph中定义好文件读取的运算节点，把Graph传入Session运行时，执行读取文件的运算，这样可以避免在Python和TensorFlow C++执行环境之间反复传递数据。
## Embeddings
TensorBoard是TensorFlow自带的一个可视化工具，Embeddings是其中的一个功能，用于在二维或三维空间对高维数据进行探索。   
挖坑
## 分布式部署
### 单机多GPU训练
单机的多GPU训练， tensorflow的官方已经给了一个cifar的例子，已经有比较详细的代码和文档介绍， 这里大致说下多GPU的过程，以便方便引入到多机多GPU的介绍。
单机多GPU的训练过程：
1. 假设你的机器上有3个GPU;
2. 在单机单GPU的训练中，数据是一个batch一个batch的训练。 在单机多GPU中，数据一次处理3个batch(假设是3个GPU训练）， 每个GPU处理一个batch的数据计算。
3. 变量，或者说参数，保存在CPU上
4. 刚开始的时候数据由CPU分发给3个GPU， 在GPU上完成了计算，得到每个batch要更新的梯度。
5. 然后在CPU上收集完了3个GPU上的要更新的梯度， 计算一下平均梯度，然后更新参数。
6. 然后继续循环这个过程。

通过这个过程，处理的速度取决于最慢的那个GPU的速度。如果3个GPU的处理速度差不多的话， 处理速度就相当于单机单GPU的速度的3倍减去数据在CPU和GPU之间传输的开销，实际的效率提升看CPU和GPU之间数据的速度和处理数据的大小。

写到这里觉得自己写的还是不同通俗易懂， 下面就打一个更加通俗的比方来解释一下：
老师给小明和小华布置了10000张纸的乘法题并且把所有的乘法的结果加起来， 每张纸上有128道乘法题。 这里一张纸就是一个batch， batch_size就是128. 小明算加法比较快， 小华算乘法比较快，于是小华就负责计算乘法， 小明负责把小华的乘法结果加起来 。   
这样小明就是CPU，小华就是GPU.   
这样计算的话， 预计小明和小华两个人得要花费一个星期的时间才能完成老师布置的题目。   
于是小明就招来2个算乘法也很快的小红和小亮。   
于是每次小明就给小华，小红，小亮各分发一张纸，让他们算乘法， 他们三个人算完了之后， 把结果告诉小明，   
小明把他们的结果加起来，然后再给他们没人分发一张算乘法的纸，依次循环，知道所有的算完。   
这里小明采用的是同步模式，就是每次要等他们三个都算完了之后， 再统一算加法，算完了加法之后，再给他们三个分发纸张。这样速度就取决于他们三个中算乘法算的最慢的那个人， 和分发纸张的速度。
#### 分布式多机多GPU训练
随着设计的模型越来越复杂，模型参数越来越多，越来越大， 大到什么程度？多到什么程度？ 多参数的个数上百亿个，   训练的数据多到按TB级别来衡量。大家知道每次计算一轮，都要计算梯度，更新参数。 当参数的量级上升到百亿量级甚至更大之后， 参数的更新的性能都是问题。 如果是单机16个GPU， 一个step最多也是处理16个batch， 这对于上TB级别的数据来说，不知道要训练到什么时候。于是就有了分布式的深度学习训练方法，或者说框架。
#### 参数服务器
在介绍tensorflow的分布式训练之前，先说下参数服务器的概念。
前面说道， 当你的模型越来越大， 模型的参数越来越多，多到模型参数的更新，一台机器的性能都不够的时候， 很自然的我们就会想到把参数分开放到不同的机器去存储和更新。   
因为碰到上面提到的那些问题， 所有参数服务器就被单独拧出来， 于是就有了参数服务器的概念。 参数服务器可以是多台机器组成的集群， 这个就有点类似分布式的存储架构了， 涉及到数据的同步，一致性等等， 一般是key-value的形式，可以理解为一个分布式的key-value内存数据库，然后再加上一些参数更新的操作。 详细的细节可以去google一下， 这里就不详细说了。 反正就是当性能不够的时候， 几百亿的参数分散到不同的机器上去保存和更新，解决参数存储和更新的性能问题。   
借用上面的小明算题的例子，小明觉得自己算加法都算不过来了， 于是就叫了10个小明过来一起帮忙算。
### tensorflow的分布式
不过据说tensorflow的分布式没有用参数服务器，用的是数据流图， 这个暂时还没研究，不过应该和参数服务器有很多相似的地方，这里介绍先按照参数服务器的结构来介绍。
tensorflow的分布式有**in-graph**和**between-gragh**两种架构模式。 这里分别介绍一下。
#### in-graph
in-graph模式和单机多GPU模型有点类似。 还是一个小明算加法， 但是算乘法的就可以不止是他们一个教室的小华，小红，小亮了。 可以是其他教师的小张，小李。。。。.   
in-graph模式， 把计算已经从单机多GPU，已经扩展到了多机多GPU了， 不过数据分发还是在一个节点。 这样的好处是配置简单， 其他多机多GPU的计算节点，只要起个join操作， 暴露一个网络接口，等在那里接受任务就好了。 这些计算节点暴露出来的网络接口，使用起来就跟本机的一个GPU的使用一样， 只要在操作的时候指定tf.device("/job:worker/task:n")， 就可以向指定GPU一样，把操作指定到一个计算节点上计算，使用起来和多GPU的类似。   
但是这样的坏处是训练数据的分发依然在一个节点上， 要把训练数据分发到不同的机器上， 严重影响并发训练速度。在大数据训练的情况下， 不推荐使用这种模式。
#### between-graph模式
between-graph模式下，训练的参数保存在参数服务器， 数据不用分发， 数据分片的保存在各个计算节点， 各个计算节点自己算自己的， 算完了之后， 把要更新的参数告诉参数服务器，参数服务器更新参数。这种模式的优点是不用训练数据的分发了， 尤其是在数据量在TB级的时候， 节省了大量的时间，所以大数据深度学习还是推荐使用between-graph模式。
#### 同步更新和异步更新
in-graph模式和between-graph模式都支持同步和异步更新   
在同步更新的时候， 每次梯度更新，要等所有分发出去的数据计算完成后，返回回来结果之后，把梯度累加算了均值之后，再更新参数。 这样的好处是loss的下降比较稳定， 但是这个的坏处也很明显， 处理的速度取决于最慢的那个分片计算的时间。
在异步更新的时候， 所有的计算节点，各自算自己的， 更新参数也是自己更新自己计算的结果， 这样的优点就是计算速度快， 计算资源能得到充分利用，但是缺点是loss的下降不稳定， 抖动大。   
在数据量小的情况下， 各个节点的计算能力比较均衡的情况下， 推荐使用同步模式；数据量很大，各个机器的计算性能掺差不齐的情况下，推荐使用异步的方式。
## eval和run
你可以使用sess.run()在同一步获取多个tensor中的值，使用Tensor.eval()时只能在同一步当中获取一个tensor值，并且每次使用 eval 和 run时，都会执行整个计算图。
## tf.InteractiveSession()与tf.Session()
tf.InteractiveSession():它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。   
tf.Session():需要在启动session之前构建整个计算图，然后启动该计算图。   
意思就是在我们使用tf.InteractiveSession()来构建会话的时候，我们可以先构建一个session然后再定义操作（operation），如果我们使用tf.Session()来构建会话我们需要在会话构建之前定义好全部的操作（operation）然后再构建会话。
## tf.add和tf.nn.bias_add和+和tf.add_n
**tf.ann.bias_add(value, bias, name=None)**作用是将偏差项bias加到value上面，可看作是tf.add的一个特例。其中bias且必须与value第二维相同，可广播。   
**tf.add**更加一般化的API，实现一个列表元素的相加，支持第二个操作数是一维的情况，可广播。   
**+运算符**和tf.add并无明显差异   
**tf.add_n([p1, p2, p3....])**实现一个列表的元素的相加。就是输入的对象是一个列表，列表里的元素可以是向量，矩阵等。不可广播。   
实例
```
import tensorflow as tf

a = tf.constant([[1, 1], [2, 2], [3, 3]], dtype=tf.float32)
b = tf.constant([1, -1], dtype=tf.float32)
c = tf.constant([1], dtype=tf.float32)

with tf.Session() as sess:
    print('bias_add:')
    print(sess.run(tf.nn.bias_add(a, b)))
    # 执行下面语句错误
    # print(sess.run(tf.nn.bias_add(a, c)))

    print('add:')
    print(sess.run(tf.add(a, c)))
```
结果如下
```
bias_add:
[[2. 0.]
 [3. 1.]
 [4. 2.]]
add:
[[2. 2.]
 [3. 3.]
 [4. 4.]]
```
实例
```

input1 = tf.constant([1.0, 2.0, 3.0])
input2 = tf.Variable(tf.random_uniform([3]))
output = tf.add_n([input1, input2])
 
with tf.Session() as sess:
	sess.run(tf.initialize_all_variables())
	print sess.run(input1 + input2)
	print sess.run(output)
```
结果如下
```
[ 1.68921876  2.73008633  3.04061747]
[ 1.68921876  2.73008633  3.04061747]
```

