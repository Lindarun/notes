# 深度学习
[TOC]
## 激活函数
### Sigmoid
#### 定义
Sigmoid 函数的一般形式是
```math
Sigmoid(x;a) = \frac{1}{1+e^{-ax}}
```
这里，参数 a 控制 Sigmoid 函数的形状，对函数基本性质没有太大的影响。在神经网络中，一般设置 a=1，直接省略。   
Sigmoid 函数的导数
```math
Sigmoid'(x;a) = Sigmoid(x;a)(1-Sigmoid(x;a))
```
#### 函数图像
![image](https://yuuki.im/uploads/images/MachineLearning/sigma-sigma-prime.jpg)
#### 性质
优点
1. 平滑曲线
2. 易于求导
3. 值域在(0, 1)内，可用作概率解释

缺点
1. 幂运算相对耗时
2. 函数平坦段容易导致梯度消失
3. 不以零为中心，可能导致模型收敛速度慢

现实应用中sigmoid函数**已被淘汰**。
### tanh
#### 定义
tanh 函数全称 Hyperbolic Tangent，即双曲正切函数。它的表达式是
```math
tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
```
其导数为
```math
tanh(x)' = 1-tanh^2(x)
```

#### 函数图像
![image](https://yuuki.im/uploads/images/MachineLearning/tanh-tanh-prime.jpg)
#### 性质
与sigmoid其实比较相似
优点
1. 易于求导
2. 曲线平滑
3. 以零为中心
缺点
1. 无法避免梯度消失的问题
2. 幂运算相对耗时

仍较少人使用，**实验性大于实用性**
### ReLu等非线性
现在最热门的一类方法，详情参见**7.31深度学习**
### 激活函数以零为中心的影响
#### 收敛速度
这里首先需要给收敛速度做一个诠释。模型的最优解即是模型参数的最优解。通过逐轮迭代，模型参数会被更新到接近其最优解。这一过程中，**迭代轮次多，则我们说模型收敛速度慢；反之，迭代轮次少，则我们说模型收敛速度快。**
#### 参数更新
深度学习一般的学习方法是反向传播。简单来说，就是通过链式法则，求解全局损失函数 L(x⃗ ) 对某一参数 w 的偏导数（梯度）；而后辅以学习率 η，向梯度的反方向更新参数 w。其公式为
```math
w \leftarrow w - η x_i \frac{\partial L}{\partial f}
```
由上可知，**参数更新的方向取决于xi的符号**。
#### 以零为中心的影响
至此，为了描述方便，我们以二维的情况为例。亦即，神经元描述为
```math
f(\vec{x}; \vec{w}, \vec{b}) = f(w_0x_0 + w_1x_1+b)
```
现在假设，参数 w0, w1 的最优解 w∗0, w∗1 满足条件
```math
\begin{cases}
w_0<w^*_0 &  \\
w_1 \ge w^*_1 & \\
\end{cases}
```
这也就是说，我们希望 w0 适当增大，但希望 w1 适当减小。考虑到上一小节提到的更新方向的问题，这就必然要求 x0 和 x1 符号相反。

但在 Sigmoid 函数中，输出值恒为正。这也就是说，如果上一级神经元采用 Sigmoid 函数作为激活函数，那么我们无法做到 x0 和 x1 符号相反。此时，模型为了收敛，不得不向逆风前行的风助力帆船一样，走 Z 字形逼近最优解。
![image](https://yuuki.im/uploads/images/MachineLearning/zig-zag-gradient.png)
如图，模型参数走绿色箭头能够最快收敛，但由于输入值的符号总是为正，所以模型参数可能走类似红色折线的箭头。如此一来，使用 Sigmoid 函数作为激活函数的神经网络，收敛速度就会慢上不少了。

## 数据预处理
### 零均值化
均值减法（Mean subtraction）是预处理最常用的形式。它对数据中每个独立特征减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码**X -= np.mean(X, axis=0)实现**。而对于图像，更常用的是对所有像素都减去一个值，可以用**X -= np.mean(X)**实现，也可以在3个颜色通道上分别操作，即三个通道上都减去对应的平均值。
### 标准化
是指将数据的所有维度都归一化，使其**数值范围都近似相等**。有两种常用方法可以实现归一化。   
第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为X /= np.std(X, axis=0)。   
第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。   
在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。    
![image](https://pic2.zhimg.com/80/e743b6777775b1671c3b5503d7afbbc4_hd.jpg)
(图片为对应预处理后的效果)
### PCA与白化
#### PCA
这里先讲操作，背后的数学知识到时候再补。   
先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的**相关性结构**。
```
# 假设输入数据矩阵X的尺寸为[N x D]
X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)
cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵
```
数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的协方差。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和半正定的。我们可以对数据协方差矩阵进**行SVD（奇异值分解）运算**。
```
U,S,V = np.linalg.svd(cov)
```
U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：

```
Xrot = np.dot(X,U) # 对数据去相关性
```
注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算Xrot的协方差矩阵，将会看到它是对角对称的。**np.linalg.svd的一个良好性质是在它的返回值U中，特征向量是按照特征值的大小排列的**。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也被称为**主成分分析（ Principal Component Analysis 简称PCA）降维**：

```
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100]
```
经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大方差的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。
#### 白化
白化操作的输入是**特征基准上的数据**，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：
```
# 对数据进行白化操作:
# 除以特征值 
Xwhite = Xrot / np.sqrt(S + 1e-5)
```
> 警告：夸大的噪声。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。

![image](https://pic1.zhimg.com/80/aae11de6e6a29f50d46b9ea106fbb02a_hd.jpg)
PCA/白化。左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。
#### 实践操作
实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。不过像素值都已经在特定范围之内，故我认为没必要归一化。
#### 常见错误
进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能**在训练集数据上**进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。应该怎么做呢？**应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。**

## 参数初始化
我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。
### 避免全零初始化
经过正向传播和反向传播后，参数的不同维度之间经过相同的更新，迭代的结果是不同维度的参数是一样的，换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。严重地影响了模型的性能。一般只在训练SLP/逻辑回归模型时才使用0初始化所有参数, 深度模型都不会使用0初始化所有参数。
### 小随机数初始化
权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来**打破对称性**。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身****变成整个网络的不同部分****。小随机数权重初始化的实现方法是：W = 0.01 * np.random.randn(D,H)。其中randn函数是基于零均值和标准差的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。   
但并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。
### Xaiver初始化
```
tf.contrib.layers.xavier_initializer_conv2d
```

上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为**w = np.random.randn(n) / sqrt(n)。其中n是输入数据的数量。**这样就保证了网络中所有神经元起始时**有近似同样的输出分布**。实践经验证明，这样做可以提高收敛的速度。
### Kaiming(He)初始化
```
tf.contrib.layers.variance_scaling_initializer
```

一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是2.0/n。代码为**w = np.random.randn(n) * sqrt(2.0/n)**。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。
### 偏置（biases）的初始化。
通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。