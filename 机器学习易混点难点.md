[TOC]
# logistic回归-最大熵模型
## 为何最大熵
熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。  
为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。换言之，在已知部分知识的前提下，**关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断**，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。   
例如，投掷一个骰子，如果问"每个面朝上的概率分别是多少"，你会说是等概率，即各点出现的概率均为1/6。因为对这个"一无所知"的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是**风险最小**的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。
## 约束条件
设联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布分别为
```math
 \widetilde{P}(X=x,Y=y) = \frac{v(X=x, Y=y)}{N}
```
```math
 \widetilde{P}(X=x) = \frac{v(X=x)}{N}
```
用特征函数f(x, y)描述输入x和输入y之间的某一个事实，其定义是
```math
f(x, y)=\begin{cases}
1 & \text{x与y满足某一事实} \\
0.5 & \text{否则} \\
\end{cases}
```
特征函数f(x,y)关于经验分布的期望值
```math
E_{\widetilde{P}} = \sum_{x, y}\widetilde{P}(x,y)f(x,y)
```
特征函数f(x,y)关于模型P(Y|X)与经验分布的期望值
```math
E_P = \sum_{x,y}\widetilde{P}(X)P(y|x)f(x,y)
```
如果模型能够获取训练数据中的信息，那么可假设这两个期望值相等
```math
E_P(f)=E_{\widetilde{P}}(f)
```
上式就作为模型学习的**约束条件**。
## 定义
假设满足所有约束条件的模型集合为
```math
C \equiv\{P\in\rho|E_P(f_i)=E_{\widetilde P}(f_i),i = 1, 2,3,...,n\}
```
定义在条件概率分布P(Y|X)上的条件熵为
```math
H(P)=-\sum_{x,y}\widetilde P(x)P(y|x)lnP(y|x)
    =-\sum_{x\in X}\widetilde P(X)\sum_{y\in Y}P(y|x)lnP(y|x)
```
则模型集合C中条件熵最大的模型称为最大熵模型。
## 最大熵模型的学习
具体看书和拉格朗日对偶性。
1.  确定最优化问题
2.  定义拉格朗日函数L(P,w)
3.  求解max_w min_P L(P,w)
4. 关于P的极小化问题（固定w）
5. 求解关于w的极大化问题
## 极大似然估计
对偶函数的极大化等价于最大熵模型的极大似然估计（书87页，证明对偶函数等价于对数似然函数）   
可以将最大熵模型写成更一般的形式
```math
P_w(y|x) = \frac{1}{Z_w(x)}exp(\sum_{i=1}^{n}w_if_i(x,y))
```
其中
```math
Z_w(x)=\sum_{y}exp(\sum_{i=1}^{n}w_if_i(x,y))
```
这里x为输入，y为输出，w为权值向量，`$f_i(x,y)$`为任意实值特征函数。
## 最优化算法
### 改进的迭代尺度法
输入：特征函数f1,f2,f3,...,fn；经验分布`$\widetilde P(X,Y)$`，模型`$P_w(y|x)$`
输出：最优参数值`$w_i^*$`；最优模型`$P_w$`
1. 对所有`$i\in\{1,2,,...,n\}$`,取初值`$w_i=0$`
2. 对每一`$i\in\{1,2,,...,n\}$`：   
（a）令
```math
\delta_i = \frac{1}{M}log\frac{E_{\widetilde P}(f_i)}{E_P(f_i)}\quad M=C
```
```math
\delta_i^{k+1}=\delta_i^{k}-\frac{g(\delta_i^k)}{g`(\delta_i^k)}
```

（b）更新`$w_i$`:`$w_i <- w_i + \delta_i$`
3. 如果不是所有wi都收敛，重复2
## 拟牛顿法

# SVM




