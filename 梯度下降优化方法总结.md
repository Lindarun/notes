[TOC]
# 梯度下降优化方法总结
## 数据用量
### BGD批量梯度下降法
在每一轮的训练过程中，Batch Gradient Descent算法用**整个训练集**的数据计算cost fuction的梯度，并用该梯度对模型参数进行更新。   
优点:
1. cost fuction若为凸函数，能够**保证**收敛到全局最优值；若为非凸函数，能够收敛到局部最优值   

缺点:
1. 由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢
2. 训练数较多时，需要较大内存
3. 批量梯度下降不允许在线更新模型，例如新增实例。
### SGD随机梯度下降法
和批梯度下降算法相反，Stochastic gradient descent 算法每读入**一个数据(随机)**，便立刻计算cost fuction的梯度来更新参数。   
优点：
1. 算法收敛速度快(在Batch Gradient Descent算法中, 每轮会计算很多相似样本的梯度, 这部分是冗余的)
2. 允许在线更新
3. 有几率跳出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优   

缺点：
1. 容易收敛到局部最优，并且容易被困在鞍点
### MBGD小批量梯度下降法
mini-batch Gradient Descent的方法是在上述两个方法中取折衷, 每次从所有训练数据中取**一个子集**（mini-batch） 用于计算梯度。在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法。

---
上述三个方法面临的主要挑战如下：
- 选择适当的学习率α 较为困难。太小的学习率会导致收敛缓慢，而学习速度太块会造成较大波动，妨碍收敛。
目前可采用的方法是在训练过程中调整学习率大小，例如模拟退火算法：预先定义一个迭代次数m，每执行完m次训练便减小学习率，或者当cost function的值低于一个阈值时减小学习率。然而迭代次数和阈值必须事先定义，因此无法适应数据集的特点。
- 上述方法中, 每个参数的 learning rate 都是相同的，这种做法是不合理的：如果训练数据是稀疏的，并且不同特征的出现频率差异较大，那么比较合理的做法是对于出现频率低的特征设置较大的学习速率，对于出现频率较大的特征数据设置较小的学习速率。
- 近期的的研究表明，深层神经网络之所以比较难训练，并不是因为容易进入local minimum。相反，由于网络结构非常复杂，在绝大多数情况下即使是 local minimum 也可以得到非常好的结果。而之所以难训练是因为学习过程容易陷入到马鞍面中，即在坡面上，一部分点是上升的，一部分点是下降的。而这种情况比较容易出现在平坦区域，在这种区域中，所有方向的梯度值都几乎是 0。
## 梯度更新算法
以上使用原始参数更新方法
```math
\Delta \theta_t = \alpha·\nabla J(\theta)
```
```math
\theta = \theta - \Delta \theta_t
```
效率低，就如同一个喝醉酒的人摇摇晃晃。
### Momentum动量
下坡就该直接控制不住走下去，管你醉没醉。   
SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的**动量**概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：
```math
\Delta \theta_t = \gamma·\Delta \theta_{t-1} + \alpha·\nabla J(\theta)
```
```math
\theta = \theta - \Delta \theta_t
```
Momentum算法会观察历史梯度(t−1)，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。一种形象的解释是：我们把一个醉汉推下山，醉汉在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若醉汉的方向发生变化，则动量会衰减。(历史梯度的积累)
```
# 动量更新
v = mu * v - learning_rate * dx # 与速度融合
x += v # 与位置融合
```
在这里引入了一个初始化为0的变量v和一个超参数mu。说得不恰当一点，这个变量（mu）在最优化的过程中被看做动量（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。


### Nesterov Accelerated Momentum优化动量
让醉汉在下坡时走得更快，走得更聪明。   
既然Momentum每一步都要将两个梯度方向（历史梯度、当前梯度）做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的“超前梯度”来做梯度合并呢？如此一来，醉汉就可以先不管三七二十一先往前走一步，在靠前一点的位置看到梯度，然后按照那个位置再来修正这一步的梯度方向。如此一来，有了超前的眼光，醉汉就会更加”聪明“, 这种方法被命名为Nesterov accelerated gradient 简称 NAG。
```math
\Delta \theta_t = \gamma·\Delta \theta_{t-1} + \alpha·\nabla J(\theta - \gamma·\Delta \theta_{t-1})
```
```math
\theta = \theta - \Delta \theta_t
```
![image](https://img-blog.csdn.net/20170805212728775?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDA4OTQ0NA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```
v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
```

### Adagrad
换个鞋子变成醉汉走弯路的阻力，逼着直走下坡。   
上述方法中，对于每一个参数θi 的训练都使用了相同的学习率α。Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。因此，Adagrad非常适合处理**稀疏数据**。   
我们设gt,i为第t轮第i个参数的梯度，即gt,i=▽ΘJ(Θi)。因此，SGD中参数更新的过程可写为
```math
\theta_{t+1, i} = \theta_{t, i} - \alpha · g_{t,i}
```
Adagrad在每轮训练中对每个参数θi的学习率进行更新，参数更新公式如下：
```math
\theta_{t+1, i} = \theta_{t, i} - \frac{\alpha}{\sqrt{G_{i, ii} + \epsilon}} ·g_{t,i}
```
其中，Gt∈Rd×d为对角矩阵，每个对角线位置i,i为对应参数θi从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束。
```
# 假设有梯度和参数向量x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```
注意，变量cache的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。
### RMSprop
不受控制与换鞋的结合。   
RMSprop是Geoff Hinton提出的一种自适应学习率方法。Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。
```math
v = \gamma v+(1-\gamma)*dx^2
```
```math
W += -\alpha \frac{dx}{\sqrt{v}}
```
```
cache =  decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```
在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中x+=和Adagrad中是一样的，但是cache变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。
### Adam
Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下： 
```math
m = \gamma_1 m + (1-\gamma_1)*dx
```
 ```math
v = \gamma_2 v+(1-\gamma)*dx^2
```
```math
W += -\alpha \frac{m}{\sqrt{v}}
```

```
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
```
