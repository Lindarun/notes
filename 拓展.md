[TOC]
# 奥卡姆剃刀定律
又称“*奥康的剃刀*”，它是由14世纪英格兰的逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。这个原理称为“如无必要，勿增实体”，即“**简单有效原理**”。
# NP
## P问题
P(Polynomial,多项式)问题.P问题是可以在多项式时间内被确定机(通常意义的计算机)解决的问题.
### 多项式时间算法
多项式算法(polynomial algorithm)亦称有效算法或好算法一类计算时间不超过始数据量的一个多项式的算法。
## NP问题
NP(Non-Deterministic Polynomial, 非确定多项式)问题,是指可以在多项式时间内被非确定机(他可以猜,他总是能猜到最能满足你需要的那种选择,如果你让他解决n皇后问题,他只要猜n次就能完成----每次都是那么幸运)解决的问题.   
这里给出NP问题的另一个定义,这种可以在多项式时间内验证一个解是否正确的问题称为NP问题，亦称为验证问题类。
### NPC：NP完全问题
npc问题，是NP的一个子集，且其中每一个问题均能由NP中的任何问题在多项式时间内转化而成。换句话说，只要解决了这个问题，那么所有的NP问题都解决了。其定义要满足2个条件： 
- 它得是一个NP问题
- 所有的NP问题都可以约化到它
## NPhard：NP难问题
它满足NPC问题定义的第二条但不一定要满足第一条（就是说，NP-Hard问题要比NPC问题的范围广，**NP-Hard问题没有限定属于NP**），即所有的NP问题都能约化到它，但是他不一定是一个NP问题。NP-Hard问题同样难以找到多项式的算法，但它不列入我们的研究范围，因为它不一定是NP问题。即使NPC问题发现了多项式级的算法，NP-Hard问题有可能仍然无法得到多项式级的算法。事实上，由于NP-Hard放宽了限定条件，它将有可能比所有的NPC问题的时间复杂度更高从而更难以解决。
![avatar](https://img-blog.csdn.net/20151015164207766)
# 凸壳
凸壳可以看作是**点集合的边界**，其精确定义如下：   
　　设集合S是n维空间的k个点组成的集合，即S={x1,x2,...xk},xi是n维向量。定义S的凸壳Conv(S)为：   
　　Conv(S)＝{x=λ1\*x1+λ2\*x2+...+λk*xk | λ1+λ2+ . . .+λk=1}
# 概率
## 条件概率
事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P（A|B），读作“在B条件下A的概率”。
## 边缘概率
边缘概率 Marginal Probability 是某个事件发生的概率，而与其它事件无关。边缘概率是这样得到的：在联合概率中，把最终结果中不需要的那些事件合并成其事件的全概率而消失（对离散随机变量用求和得全概率，对连续随机变量用积分得全概率）。这称为边缘化（marginalization）。A的边缘概率表示为P（A），B的边缘概率表示为P（B）。
## 完备事件组
设S为试验E的样本空间，B1，B2，…，Bn为E的一组事件。若

- Bi ∩ Bj=∅ (i≠j且i、j=1，2，…，n);

- B1∪B2∪…∪Bn=S，

则称B1，B2，…，Bn为样本空间S的一个完备事件组。
### 全概率公式
如果事件B1、B2、B3…Bn 构成一个完备事件组，即它们两两互不相容，其和为全集;并且P(Bi)大于0，则对任一事件A有
```math
P(A)=P(A|B1)*P(B1) + P(A|B2)*P(B2) + ... + P(A|Bn)*P(Bn).
```
## 先验概率
先验概率仅仅依赖于主观上的经验估计，也就是事先根据已有的知识的推断，先验概率就是没有经过实验验证的概率，根据已知进行的主观臆测。
## 后验概率
后验概率是指在得到“结果”的信息后重新修正的概率，如贝叶斯公式中的。是“执果寻因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。解释下来就是，在已知果（B）的前提下，得到重新修正的因（A）的概率P（A|B)，称为A的后验概率，也即条件概率。后验概率可以通过贝叶斯公式求解。
## 似然函数
统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。   
似然函数的重要性不是它的具体取值，而是**当参数变化时函数到底变小还是变大**。对同一个似然函数，如果存在一个参数值，使得它的函数值达到**最大**的话，那么这个值就是**最为“合理”的参数值**。这方法也称**极大似然估计法**。 
# 协方差
协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。

```math
Cov(X,Y) = E(XY)-E(X)E(Y)
```
如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。 如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。
如果X与Y是统计独立的，那么二者之间的协方差就是0，这是因为

```math
E(X)E(Y) = E(XY)
```
但是反过来并不成立，即如果X与Y的协方差为0，二者并不一定是统计独立的。   
取决于协方差的相关性η(也称**相关系数**):
> 相关系数可看做一种剔除了两个变量量纲影响、标准化后的特殊协方差。

```math
η = \frac{cov(X,Y)}{\sqrt(var(X)var(Y))}
```
更准确地说是线性相关性，是一个衡量线性独立的无量纲数，其取值在[－1, 1]之间。相关性η = 1时称为“完全线性相关”（相关性η = －1时称为“完全线性负相关”），此时将Yi对Xi作Y-X 散点图，将得到一组精确排列在直线上的点；相关性数值介于－1到1之间时，其绝对值越接近1表明线性相关性越好，作散点图得到的点的排布越接近一条直线。   
方差与协方差的关系
```math
var(X) = cov(X,X)
```
# 信息论
设训练数据集为D。|D|表示其样本容量即样本总数，设有K个类Ck，|Ck|为属于类标签Ck的样本总数，总和即为|D|。设特征A有n个不同的取值{A1，A2，...An}，根据特征A的取值将D划分为n个子集D1，D2，...Dn，总和也为|D|。计子集Di中属于类Ck的样本集合为Dik。
## 信息熵
在信息论和概率统计中，**熵表示随机变量不确定性的度量**。设X是一个取有限个值的离散随机变量，其概率分布为
```math
P(X=x_i) = p_i, i = 1,2,...,n
```
则随机变量X的熵定义为

```math
H(X) = -\sum_{i=1}^np_ilogpp_i=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{D}
```
### 最大熵定理证明
对于有限离散随机变量集合，当集合中的事件等概率发生时，熵达到最大值。可由**KL散度(KL相对熵)不等式**证明：   
设可能结果有n个，P(x)为随机分布，Q(x)为均匀分布
```math
\begin{array}{l}
\quad KL(P||Q) = \sum_{x}P(x)log\frac{P(x)}{Q(x)} \\
\quad \quad \quad \quad \quad \quad= \sum_x P(x)logP(x)-\sum_x P(x)log\frac{1}{n} \\
\quad \quad \quad \quad \quad \quad= -H(P)+logn\geq0 \\
\end{array}
```
证毕。
## 条件熵
H（Y|X）表示**在已知随机变量X的条件下随机变量Y的不确定性**，随机变量X在给定的条件下随机变量Y的条件熵定义为X在给定条件下Y的条件概率分布的熵对X的数学期望   
设有随机变量(X,Y)，其联合概率分布为
```math
P(X=x_i,Y=y_j) = p_{ij}
```
```math
H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)
H(D|A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D|}log_2\frac{|D_{ik}|}{|D_i|}
```
> 令0log0 = 0
> 这里的log可为log2（单位为比特bit）或ln（单位为纳特nat）

## 互信息
量化的度量信息与要研究的事物的相关性。   
假设有两个随机事件X和Y，他们的互信息的定义如下：
```math
I(X;Y)=\sum_{x\in X,y\in Y}P(x,y)log\frac{P(x,y)}{P(x)P(y)}
```
或者更为直观的
```math
I(X;Y)=H(X)-H(X|Y)
```
由以上可得，所谓相关性的量化度量，就是在了解了其中一个Y的前提下，对另一个X不确定性所提供的信息量。值域为从0到min(H(X),H(Y))。
## 相对熵
### 定义
又称“交叉熵”，KL散度，它用来衡量两个取值为正数的函数的相似性；在概率的角度，它可以度量两个随机变量的距离，分布的差异。
```math
KL(f(x)||g(x))=\sum_{x\in X}f(x)log\frac{f(x)}{g(x)}
```
### 性质
一、对于两个完全相同的函数(分布)，它们的相对熵等于0   
二、相对熵越大，两个函数差异越大；反之越小   
三、相对熵不满足对称性，即
```math
KL(f(x)||g(x))\neq KL(g(x)||f(x))
```
为了对称，琴生和香农提出一种新的相对熵计算方法，就是取平均
```math
JS(f(x)||g(x))=\frac{1}{2}(KL(f(x)||g(x))+ KL(g(x)||f(x)))
```
四、相对熵为非负值   
证明需要用到吉布斯不等式，其描述如下：   
![image](http://images.cnitblog.com/blog/571227/201501/072057490465187.png)
## 信息增益
特征A对训练数据集D的信息增益g(D,A)，定义为
```math
g(D,A) = H(D)-H(D|A)
```
经验熵H(D)表示对数据集D进行分类的不确定性，而经验条件熵H(D|A)表示在特征A给定条件下对数据集D进行分类的不确定性。则他们的差信息增益，就**表示由于特征A而使得对数据集D的分类的不确定性减少的程度**。信息增益依赖于特征。用于ID3特征选择算法
## 信息增益比
**以信息增益作为划分原则，存在偏向于选择取值较多的特征的问题**。使用信息增益比可以对这一问题进行校正，这是特征选择的另一准则。定义为：
```math
g_R(D,A) = \frac{g(D,A)}{H_A(D)}
```
其中
```math
H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
```
n是特征A的取值个数，表示的是训练数据集D关于特征A的值的熵。
# 仿射函数
仿射函数即由由1阶多项式构成的函数,一般形式为
```math
 f (x) = A x + b,
```
这里,A 是一个 m×k 矩阵,x 是一个K向量,b是一个m向量,实际上反映了一种从 k 维到 m 维的空间映射关系.   
设f是一个矢性（值）函数,若它可以表示为
```math
f(x_1,x_2,…,x_n)=A_1x_1+A_2x_2+…+A_nx_n+b
```
其中Ai可以是标量,也可以是矩阵,则称f是仿射函数.
其中的特例是,标性（值）函数f(x)=ax+b,其中a、x、b都是标量.此时严格讲,只有b=0时,仿射函数才可以叫“线性函数”（“正比例”关系）.
# 凸优化
**凸优化**是指一种比较特殊的优化，是指求取最小值的目标函数为凸函数的一类优化问题。其中，目标函数为凸函数且定义域为凸集的优化问题称为无约束凸优化问题。而目标函数和不等式约束函数均为凸函数，等式约束函数为仿射函数，并且定义域为凸集的优化问题为约束优化问题 。   
不严格的说，凸优化就是在标准优化问题的范畴内，要求目标函数和约束函数是凸函数的一类优化问题。      
简单的说，优化问题中，目标函数为凸函数，约束变量取值于一个凸集中的优化问题称为凸优化，举个简单例子，设S为凸集，f(x)为S上凸函数，则问题min f(x) s.t. x属于S为一个凸优化。 
## 凸集
在欧氏空间中，凸集是对于集合内的每一对点，连接该对点的直线段上的每个点也在该集合内。例如，立方体是凸集，但是任何中空的或具有凹痕的例如月牙形都不是凸集。   
![image](https://images0.cnblogs.com/blog/381513/201309/03232243-1206b69427214f38875b19493cf9bc19.png)   
或者有琴生不等式定义：假设S为在实或复向量空间的集。若满足
```math
(1-t)x+ty \in S \quad \forall x, y\in S,\forall t \in (0, 1)
```
则称S为**凸集**。



